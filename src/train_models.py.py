# -*- coding: utf-8 -*-
"""CLassic_DS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FfeOwAOEhPUHywc2HPpQqEnLHELubO7l
"""

import kagglehub
path = kagglehub.dataset_download("lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")
print("Path to dataset files:", path)

import pandas as pd
df = pd.read_csv('/root/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1/IMDB Dataset.csv')
print(df.head())

import kagglehub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.cluster import KMeans
from sklearn.metrics import classification_report, accuracy_score, silhouette_score
from wordcloud import WordCloud
import spacy
from spacy.lang.en.stop_words import STOP_WORDS

# Загрузка данных
path = kagglehub.dataset_download("lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")
print("Path to dataset files:", path)
df = pd.read_csv(f'{path}/IMDB Dataset.csv')

# Предобработка текста с использованием spaCy
nlp = spacy.load("en_core_web_sm", disable=["parser", "ner"])  # Отключаем parser и NER для скорости

def preprocess_text_spacy_pipe(texts):
    """Предобработка текста: лемматизация и удаление стоп-слов с использованием spaCy."""
    clean_texts = []
    for doc in nlp.pipe(texts, batch_size=50, n_process=-1):  # Многопроцессорная обработка
        tokens = [token.lemma_ for token in doc if token.is_alpha and token.text not in STOP_WORDS]
        clean_texts.append(" ".join(tokens))
    return clean_texts

df["clean_text"] = preprocess_text_spacy_pipe(df["review"])
df["review_length"] = df["clean_text"].apply(lambda x: len(x.split()))  # Длина отзывов после очистки

# Визуализация: облако слов
vectorizer = TfidfVectorizer(stop_words="english", max_features=100)
X = vectorizer.fit_transform(df['clean_text'])
words = vectorizer.get_feature_names_out()
frequencies = X.sum(axis=0).A1
word_freq = dict(zip(words, frequencies))
wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(word_freq)
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.title("Word Cloud of Movie Reviews")
plt.show()

# Визуализация: распределение длины отзывов
plt.figure(figsize=(10, 5))
plt.hist(df['review_length'], bins=50, color='skyblue', edgecolor='black')
plt.title('Distribution of Review Lengths')
plt.xlabel('Review Length (words)')
plt.ylabel('Frequency')
plt.show()

# Удаление выбросов с помощью IQR
Q1 = df['review_length'].quantile(0.25)
Q3 = df['review_length'].quantile(0.75)
IQR = Q3 - Q1
df_cleaned = df[(df['review_length'] >= (Q1 - 1.5 * IQR)) & (df['review_length'] <= (Q3 + 1.5 * IQR))]

# Визуализация до/после удаления выбросов
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df['review_length'], kde=True)
plt.title('Before Outlier Removal')

plt.subplot(1, 2, 2)
sns.histplot(df_cleaned['review_length'], kde=True)
plt.title('After Outlier Removal')
plt.tight_layout()
plt.show()

# Подготовка данных для моделей
X = df_cleaned['clean_text']  # Очищенные тексты
y = df_cleaned['sentiment'].map({'positive': 1, 'negative': 0})  # Числовые метки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Преобразование текста в TF-IDF
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Функция для обучения и оценки модели
def train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name=""):
    """Обучение модели и вывод метрик производительности."""
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"Model: {model_name or model.__class__.__name__}")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    print("\n" + "-"*60 + "\n")

# 1. Логистическая регрессия с регуляризацией и настройкой гиперпараметров
param_grid_lr = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}
log_reg = LogisticRegression(max_iter=1000)
grid_lr = GridSearchCV(log_reg, param_grid_lr, cv=5, scoring='accuracy', n_jobs=-1)
grid_lr.fit(X_train_tfidf, y_train)
train_and_evaluate(grid_lr.best_estimator_, X_train_tfidf, X_test_tfidf, y_train, y_test, "Logistic Regression (Tuned)")

# 2. Дерево решений с настройкой гиперпараметров
param_grid_dt = {'max_depth': [5, 10, 20], 'min_samples_split': [2, 5, 10]}
decision_tree = DecisionTreeClassifier(random_state=42)
grid_dt = GridSearchCV(decision_tree, param_grid_dt, cv=5, scoring='accuracy', n_jobs=-1)
grid_dt.fit(X_train_tfidf, y_train)
train_and_evaluate(grid_dt.best_estimator_, X_train_tfidf, X_test_tfidf, y_train, y_test, "Decision Tree (Tuned)")

# 3. KNN с настройкой гиперпараметров
param_grid_knn = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}
knn = KNeighborsClassifier()
grid_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)
grid_knn.fit(X_train_tfidf, y_train)
train_and_evaluate(grid_knn.best_estimator_, X_train_tfidf, X_test_tfidf, y_train, y_test, "KNN (Tuned)")

# 4. Наивный Байес (без настройки, так как гиперпараметров мало)
naive_bayes = MultinomialNB()
train_and_evaluate(naive_bayes, X_train_tfidf, X_test_tfidf, y_train, y_test)

# 5. K-Means с оценкой через Silhouette Score
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X_train_tfidf)
kmeans_labels = kmeans.predict(X_test_tfidf)
silhouette = silhouette_score(X_test_tfidf, kmeans_labels)
print("K-Means Clustering:")
print("Silhouette Score:", silhouette)
print("Accuracy (comparison with true labels):", accuracy_score(y_test, kmeans_labels))
print("\n" + "-"*60 + "\n")

import joblib

models = {
    "Logistic Regression": grid_lr.best_estimator_,
    "Decision Tree": grid_dt.best_estimator_,
    "KNN": grid_knn.best_estimator_,
    "Naive Bayes": naive_bayes,
    "KMeans": kmeans
}

# Сохранение векторайзера и моделей
joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')
for name, model in models.items():
    joblib.dump(model, f'{name.lower().replace(" ", "_")}_model.pkl')
    print(f"Saved {name} model to '{name.lower().replace(' ', '_')}_model.pkl'")

# Пример загрузки и использования позже
loaded_vectorizer = joblib.load('tfidf_vectorizer.pkl')
loaded_lr_model = joblib.load('logistic_regression_model.pkl')

# Тест на новых данных
new_reviews = ["Amazing film", "Hated it"]
new_tfidf = loaded_vectorizer.transform(new_reviews)
predictions = loaded_lr_model.predict(new_tfidf)
print("Predictions for new reviews:", predictions)

!pip install gradio
import gradio as gr
import joblib

vectorizer = joblib.load("/content/tfidf_vectorizer.pkl")
model = joblib.load("/content/logistic_regression_model.pkl")

def predict_sentiment(review):
    review_tfidf = vectorizer.transform([review])
    prediction = model.predict(review_tfidf)[0]
    return "Positive" if prediction == 1 else "Negative"

interface = gr.Interface(fn=predict_sentiment, inputs="text", outputs="text")
interface.launch(share=True)





